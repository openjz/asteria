# 线性代数的几何意义


## 参考

- [b 站 3Blue1Brown-线性代数的本质](https://space.bilibili.com/88461692/channel/seriesdetail?sid=1528927)
- [https://github.com/3b1b/manim](https://github.com/3b1b/manim)

## 向量

向量（vector）的起点永远是原点（origin），因此我们将向量表示为终点的坐标

$$\vec{a}=
\begin{bmatrix}
1\cr
2\cr
3\cr
\end{bmatrix}
$$

一个向量可以理解为

- 一个数字列表
- 一个带方向的线段
- 一个点在多个方向上运动的叠加

向量的基本操作

- 数乘，$\vec{b} = 2\vec{a}$，这里$2$是一个标量（scalar），代表对向量的拉伸或压缩（scaling）
- 相加，$\vec{c} = \vec{a} + \vec{b}$，如果将向量 a 的终点作为向量 b 的起点，那么 b 就是相对于 a 的偏移量，此时可以将向量相加理解为分别沿向量 a 和 b 的两次连续的运动

## 线性组合

一个向量可以通过其他向量的数乘和相加来表示

- $\vec{c} =3 \vec{a} + 4\vec{b}$

我们把 c 称为 a 和 b 的线性组合（linear combination）

任何一个向量都可以表示为另一些向量的线性组合，例如在一个二维坐标系中如果有 $\vec{a}=\begin{bmatrix}2 \cr3 \end{bmatrix}$

那么我们可以将 a 表示为

$$\vec{a}=
2\begin{bmatrix}1\cr0\cr\end{bmatrix}
+
3\begin{bmatrix}0\cr1\cr\end{bmatrix}
$$

其中 $\begin{bmatrix}1\cr0\cr\end{bmatrix}$ 和 $\begin{bmatrix}0\cr1\cr\end{bmatrix}$ 是两个特殊的向量，我们称它们为单位向量，一般表示为 $\vec{i}=\begin{bmatrix}1\cr0\cr\end{bmatrix}$ 和 $\vec{j}=\begin{bmatrix}1\cr0\cr\end{bmatrix}$，因此，上面的 a 向量就可以表示为

$$
\vec{a} = 2\vec{i}+3\vec{j}
$$

## 线性空间

利用 i 和 j 向量的线性组合，我们可以得到无数个向量，这些向量构成了一个平面，我们把这些向量的集合称为由 i 和 j 张开的线性空间（span）。

如果 i 和 j 是同方向的向量，例如，$\vec{i} = 5\vec{j}$，那么由 i 和 j 构成的所有向量就无法张开成一个平面，只能汇聚在一条直线上，此时的线性空间就是一条直线。

## 基

我们把能张开一个线性空间的一组向量称为线性空间的一组基（basis vectors）

上面的 i 和 j 向量就构成了一组基（构成一组基的也可以是别的向量）。对于一个向量 $\vec{a}=\begin{bmatrix}x\cr y\cr\end{bmatrix}$，我们默认它所在线性空间的基是 i 和 j 向量。

## 线性相关和线性无关

如果一个向量 a 能够表示为另外一些向量的线性组合，那么 a 一定在另外那些向量张成的空间内，此时我们称 a 和另外一些向量是线性相关的（linearly dependent），否则是线性无关的（linearly independent）。

那么显然，如果基中有两个线性无关的向量，它们张成的线性空间就是一个二维平面，如果只有一个线性无关的向量，它们张成的线性空间只能是一条一维的直线，如果有三个线性无关的向量，那么它们张成的线性空间就是一个三维的空间

## 线性变换和矩阵

线性变换（linear transformation）是对一个向量做放缩、旋转等操作。

我们用对基向量的线性变换来表示对线性空间内所有向量的线性变换。通常将其表示为一个矩阵（matrix），矩阵的每一列都表示一个变换后的基向量。

对矩阵和向量做矩阵乘法，就完成了对向量的线性变换，实际上就是用新的基向量的线性组合来表示变换后的向量。

## 矩阵乘法

对矩阵乘法有两种理解

- 将两次连续的变换合并为一个
- 将右边矩阵的每一列看成一个向量，矩阵乘法是要对这些向量做线性变换

## 行列式

行列式（determinant）是想表达线性变换的拉伸程度，在 2 维向量空间，行列式的值表示新的基向量围成的平行四边形面积，在 3 维向量空间，行列式的值表示新的基向量围成的平行六面体的体积

如果一个线性变换将一个高维的线性空间压缩成低维的线性空间，行列式的值就为 0

行列式的值为负时，在一个二维线性空间内表示线性变换将平面做了一次翻转

## 线性方程组

线性方程组实际上是想通过目标向量和线性变换矩阵求出原向量

线性方程组的解有三种情况

1. 行列式的值不为 0，此时有唯一的解和目标向量对应，可以对矩阵求逆，然后对目标向量做逆变换
    1. 此时，不可能有非零向量变换为零向量。只有零向量可以变换为零向量，也只能变换为零向量。（因为线性变换要保证零向量不变） 
2. 行列式的值为 0，此时说明线性变换对向量空间做了压缩，这时我们无法对矩阵求逆，因为变换后的向量可以映射到一组原向量。
   1. 如果目标向量不在变换后的线性空间内，那么方程组无解
   2. 如果目标向量在变换后的线性空间内，那么方程组有一组解（无穷多个解）

## 秩

矩阵的秩（rank）用来表示一个经过线性变换后的新空间是几维的。

另外，如果一个三维空间被压缩到二维，那么原来的整个空间会映射为一个面，原来的一个面会映射为一条线，原来的一条线会映射为一个点（也就是一个向量）。

如果一个三维空间直接被压缩到一维，那么原来的整个空间会映射为一条线，原来的一个面会映射为一个点。

我们把线性变换后的得到的空间称为列空间（column space），即由矩阵列张成的空间，一般用变换矩阵表示列空间，因此秩的更准确的定义是列空间的维数。

变换后，零向量对应的原向量构成的空间称为零空间（null space）或核（kernel）

## 非方阵

对于一个 m>n 的矩阵，例如

$$
A =
\begin{bmatrix}
1 & 3 \cr
2 & 2\cr
3 & 1\cr
\end{bmatrix}
$$

它的作用是将一个 2 维向量映射到三维空间，其实很容易理解，把矩阵的两列看作是线性空间的基向量，由于只有两个基向量，它们只能张开一个二维的线性空间，但它们本身是三维的，因此它们在一个三维坐标系中张开了一个二维线性空间（是一个平面，有可能是斜着的平面）。矩阵有两列意味着原空间是二维的，矩阵有三行意味着新空间位于一个三维坐标系中。矩阵 A 仍然是个满秩矩阵。

类似的，对于一个 m<n 的矩阵，例如

$$B =
\begin{bmatrix}
1 & 2 & -1 \cr
2 & 3 & 1\cr
\end{bmatrix}
$$

它的作用是将一个三维向量映射到一个二维线性空间中去，也很容易理解，矩阵有三列意味着原空间是三维的，矩阵有两行意味着新空间位于一个二维坐标系中，同时也将线性空间压缩到了二维（换句话说，矩阵 B 必然不满秩）

更极端一点，我们可以把几个一维向量映射到四维坐标系中，即

$$
\begin{bmatrix}1\cr0\cr0\cr0\end{bmatrix}
\begin{bmatrix}1 & 2 & 3\end{bmatrix}=
\begin{bmatrix}
1 & 2 & 3\cr
0 & 0 & 0\cr
0 & 0 & 0\cr
0 & 0 & 0\cr
\end{bmatrix}
$$

或者，把一个四维向量映射到一维空间中去，即

$$
\begin{bmatrix}1 & 2 & 3 & 4\end{bmatrix}
\begin{bmatrix}1\cr2\cr3\cr4\end{bmatrix}=
\begin{bmatrix}
30\cr
\end{bmatrix}
$$

## 向量的点积

点积（dot products）是对两个相同维度向量的各维度分别求积，然后将各个积相加。

向量互相垂直时，点积为 0，方向相反时，点积为负

点积和顺序无关，即 $\vec{a}·\vec{b}$ 和 $\vec{b}·\vec{a}$ 是等价的。

点积可以理解为向量 a 向向量 b 投影，并用 a 投影后的长度乘以 b 的长度。

向量的点积实际上可以表示为一个 1×n 的矩阵和一个 n×1 的矩阵乘法，前面我们讲过，这个乘法实际上是做了一个从 n 维线性空间到 1 维线性空间的压缩操作，并将一个 n 维向量映射为 1 维向量，左边的 1×n 的矩阵定义了映射规则。

那么，上述的两个操作——向量投影和空间压缩，为什么会等价，即，为什么会存在下述等式？

$$
\vec{a}^\top\vec{b}=
\left|\vec{a}\right|·
\left|\vec{b}\right|·
cosθ
$$

从根本上来讲，是思维方式的不同

- 等式右侧是用几何的思想来解决问题
- 等式左侧的矩阵乘法是把向量 b 拆分成不同的维度，计算每个维度映射到向量 a 上的长度（向量 a 相当于指定了映射的倍率），最后将各维度的映射长度求和。
  - 从线性变换的角度来看，向量 a 的每一维都是一个新的位于一维空间上的基向量，而它们的值恰好等于最终合成的基向量在对应维度的投影，这是一种对偶性（duality）
  - 向量 a 的每个分量实际上是一个倍率，代表原空间的每一维对一维空间向量值的贡献。从这个意义上来看，线性变换有另外一种理解，即左边的矩阵实际上定义的是原向量的每一维向目标向量的每一维映射的倍率

## 向量的叉积

在二维空间中，严格来讲是求不了的，硬要求的话，叉积（cross product）是求两个向量围成的平行四边形面积。值和顺序有关，交换两个向量会导致结果取反，可以认为是平行四边形有方向。所以二维叉积就是求行列式的值。

在三维空间中，叉积是从两个向量生成一个和它们垂直的新向量，新向量的长度是两个向量围成的平行四边形面积。新向量的方向取决于两个向量相乘的顺序。例如

$$\vec{v}×\vec{w}= \vec{u}$$

向量 u 的方向满足右手定则（右手定则有两种，一种是将拇指、食指和中指分别指向三个互相垂直的方向，另外一种是右手握拳，伸出大拇指，这里的右手定则指的是第二种），右手握拳时，四指沿从 $\vec{v}$ 至 $\vec{w}$ 的方向握住，此时大拇指指向的就是向量 $\vec{u}$ 的方向。这也意味着，叉积是不满足交换律的，即 $\vec{v}×\vec{w} \neq \vec{w}×\vec{v}$，实际上 $\vec{v}×\vec{w} = -(\vec{w}×\vec{v})$

（上面关于右手定则的解释是从games 101第2课上学到的）

同样可以将叉积表示为求行列式

$$
\begin{bmatrix}v_1\cr v_2\cr v_3\end{bmatrix}
\begin{bmatrix}w_1\cr w_2\cr w_3\end{bmatrix}=
det\left(
\begin{bmatrix}
\vec{i} & v_1 & w_1\cr
\vec{j} & v_2 & w_2\cr
\vec{k} & v_3 & w_3\cr
\end{bmatrix}
\right)
=\vec{i}(v_2w_3-v_3w_2)+\vec{j}(v_3w_1-v_1w_3)+\vec{k}(v_1w_2-v_2w_1)
$$

把基向量写到行列式里，结果正好是个向量，这是个符号上的技巧。

如果要从几何的角度理解这个等式，可以这样理解，向量 u 是这样一个向量，任何一个向量 $[x,y,z]$ 和 u 做点积，结果都是 $[x,y,z]$ 和 v、w 围成的平行六面体的体积。所以，只有和 v、w 垂直，且长度等于 v、w 围成平行四边形的向量才能满足要求（由于 u 和 v、w 垂直，向量 $[x,y,z]$ 投影到 u 上后就是六面体的高，u 的长度又是六面体的底面积，因此 u 和 $[x,y,z]$ 做点积刚好是六面体的体积）。可以将 u 和 v、w 的关系描述成

$$
\begin{bmatrix}u_1\cr u_2\cr u_3\end{bmatrix}
·
\begin{bmatrix}x\cr y\cr z\end{bmatrix}=
det\left(
\begin{bmatrix}
x & v_1 & w_1\cr
y & v_2 & w_2\cr
z & v_3 & w_3\cr
\end{bmatrix}
\right)
$$

按这个理解，把行列式中的第一列换成 i、j、k 后就刚好能表示成所求的 u

## 基变换

基变换其实就是线性变换，只不过基变换强调的是不同基坐标系下的语义转换。例如

$$
\begin{bmatrix}2 & -1\cr 1 & 1\end{bmatrix}
\begin{bmatrix}-1 \cr 2\end{bmatrix}=
\begin{bmatrix}-4 \cr 1\end{bmatrix}
$$

假如Jennifer定义了她自己的基向量，那么向量 $[2,1]$ 和 $[-1,1]$ 代表Jennifer的基向量在我们标准坐标系下的表示，那么上面这个转换是把Jennifer坐标系下的 $[-1,2]$ 转换为我们标准坐标系下的向量。

如果我们标准坐标系下的线性变换转换为Jennifer坐标系下的表示，可以这么做，先做一个基变换（从Jennifer坐标系转换为标准坐标系），然后做一个线性变换，最后使用基变换的逆变换将其转回Jennifer坐标系。例如，如果我们想把一个标准坐标系下的左旋转90度的操作表示为Jennifer坐标系下的表示，那么最终是这样的形式

$$
\begin{bmatrix}2 & -1\cr1 & 1\end{bmatrix}^{-1}
\begin{bmatrix}0 & -1\cr1 & 0\end{bmatrix}
\begin{bmatrix}2 & -1\cr1 & 1\end{bmatrix}=
\begin{bmatrix}1/3 & -2/3 \cr 5/3 & -1/3\end{bmatrix}
$$

注意，等式右边的变换对Jennifer来说并不是左旋90度，可能是一个什么奇怪变换，只是它最终的效果是实现了标准坐标系下的左旋90度。

可以将其表示为一个通用的形式，其中M是标准坐标系下的线性变换，A是从目标坐标系到标准坐标系的基变换。

$$A^{-1}MA=T$$

简而言之，这玩意儿的作用是，我可以在目标坐标系下实现标准坐标系下的某种变换。

如果反过来，我想在标准坐标系下实现目标坐标系下的某种转换，那么可以使用M做转换

$$M = ATA^{-1}$$

## 特征向量和特征值

对于有些变量来说，线性变换仅仅使它们产生了一个拉伸效果，也就是说，它们在线性变换后和变换前位于同一条直线上（向量在变换后仍然位于变换前张成的空间上），我们把这些向量称作特征向量(eigenvector)。

特征值（eigenvalue）用来衡量线性变换对特征向量的放缩比例。

例如，对于一个三维空间中的旋转操作，旋转轴就是特征向量，因为在旋转前后，旋转轴是不会变的，而且旋转操作不会放缩任何向量，因此特征值为1。

特征向量和特征值有几种情况

1. 不存在特征向量，例如二维平面上的左旋90度操作，只会得到一个值为复数的特征值。
2. 只有一个特征值，但是特征向量不止在一条直线上。比如一个把所有向量长度延长一倍的线性变换，所有向量都是特征向量。
3. 有多个特征值，每个特征值对应不同的直线。例如线性变换 $\begin{bmatrix}3 & 0\cr0 & 2\end{bmatrix}$，x和y轴方向的向量在变换后不会改变方向，x轴方向的特征向量对应的特征值为3，y轴方向的特征向量对应的特征值是2。
4. 不会出现一个直线对应多个特征值的情况，很容易理解，我们不可能把一个向量同时放大1倍或2倍。

## 矩阵的相似对角化

对于一个线性变换，如果它有多个特征向量，并且多到能用特征向量组成一组基，那么我们可以把这个线性变换和基变换相结合，得到它在新基下的表示。由于线性变换只会对特征向量产生放缩效果，那么以特征向量作为基后表示出的线性变换必然是一个对角矩阵。这会简化很多矩阵计算。

这组由特征向量构成的基向量称为特征基（eigenbasis）

例如，现在有一个线性变换 $A=\begin{bmatrix}3 & 1\cr0 & 2\end{bmatrix}$，它的特征值为2和3，由此可以得到两个特征向量，特征值为2时，对应的特征向量为 $\begin{bmatrix}-1\cr1\end{bmatrix}$，特征值为3时，对应的特征向量为 $\begin{bmatrix}1\cr0\end{bmatrix}$，那么我们这个线性变换转换为特征基下的表示，结果恰好为一个对角矩阵。

$$
\begin{bmatrix}-1 & 1\cr1 & 0\end{bmatrix}^{-1}
\begin{bmatrix}3 & 1\cr0 & 2\end{bmatrix}
\begin{bmatrix}-1 & 1\cr1 & 0\end{bmatrix}=
\begin{bmatrix}2 & 0 \cr 0 & 3\end{bmatrix}
$$

只有当特征向量的个数足够多，能够构成一个基的时候，才能做这种变换。

## 抽象向量空间

如果一个东西具有可加性和成比例性，就可以把它当成向量来操作，例如一个函数。

如果一个运算满足以下特性，我们会把它叫做线性运算，例如函数求导。

$$L(c\vec{v}) = cL(\vec{v})$$
$$L(\vec{v}+\vec{w}) = L(\vec{v}) + L(\vec{w})$$

有时候会把这种线性运算叫做线性算子（linear operators）

例如，我们可以把一个多项式函数表示为向量，由于多项式函数总是表示成x的幂的线性组合的形式，因此可以将基函数定义为$$1,x,x^2,x^3...$$

此时，我们可以将多项式求导操作表示成一个线性变换

$$
\begin{bmatrix}
0 & 1 & 0 & 0 & ...\cr
0 & 0 & 2 & 0 & ...\cr
0 & 0 & 0 & 3 & ...\cr
0 & 0 & 0 & 0 & ...\cr
... & ... & ... & ... & ...
\end{bmatrix}
$$

其实很容易求，还是利用线性代数的思维，把矩阵的每一列看作是一个新的基函数，因此，只要对原来的每个基函数求导，就能得到新的基函数

线性代数和函数的对应关系

|线性代数|函数|
|---|---|
|线性变换（linear transforamtion）|线性算子（linear operator）|
|点积（dot product）|内积（inner product）|
|特征向量（eigenvector）|特征函数（eigenfunction）|

为了能正确的对一个东西做规范，搞清楚它究竟能不能视为一个向量空间，我们必须对向量定义一堆规则（向量加法要满足结合律和交换律、存在向量加法的单位元、标量乘法对向量加法满足分配律、标量乘法存在单位元等等），这些规则称为公理（axioms），一共八条，只有满足这八条公理，才能把一个东西当作向量空间来处理。下面列出这八条公理

1. $\vec{u}+(\vec{v}+\vec{w}) = (\vec{u}+\vec{v})+\vec{w}$
2. $\vec{v}+\vec{w} = \vec{w}+\vec{v}$
3. $存在一个零向量，使得 0 + \vec{v} = \vec{v}$
4. $对于任意向量\vec{v}，存在逆向量-\vec{v}，使得\vec{v}+(-\vec{v} = 0)$
5. $a(b\vec{v}) = (ab)\vec{v}$
6. $1\vec{v} = \vec{v}$
7. $a(\vec{v}+\vec{w}) = a\vec{v} + a\vec{w}$
8. $(a+b)\vec{v} = a\vec{v}+b\vec{v}$



